{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Doc-classi",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJXS_dx6zVvV"
      },
      "source": [
        "**Short text classification (Customer Reviews)**  \r\n",
        "\r\n",
        "In this assignment our aim is to predict a rating based on a given review. The reviews are text reviews, and the training dataset has been made available in this directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phQ4fIInz-co"
      },
      "source": [
        "Pretrained BERT_base is used in this Assignment.\r\n",
        "While trainig, all parameters of bert were kept freezed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mu_0LGcLa5Q",
        "outputId": "bd7c320c-a717-4655-839a-b89dc678f041"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0mWBXW_UJ7-",
        "outputId": "b8d11e96-bca6-47e7-c4ed-f978cd65246c"
      },
      "source": [
        "f = open(\"/content/drive/MyDrive/dlnlp-3/digital_music_5.x.train.txt\")\n",
        "data = []\n",
        "for line in f:\n",
        "  data.append(line)\n",
        "f.close()\n",
        "print(len(data))\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/dlnlp-3/digital_music_5.y.train.txt\")\n",
        "labels = []\n",
        "for line in f:\n",
        "  labels.append(float(line.split(\"\\n\")[0]))\n",
        "f.close()\n",
        "print(len(labels))\n",
        "print(labels[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58235\n",
            "58235\n",
            "[4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 5.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T82eaYRsnprb",
        "outputId": "4fe7718d-02d4-4aae-800d-f7a008e12592"
      },
      "source": [
        "for i in range(len(labels)):\n",
        "  labels[i]-=1\n",
        "\n",
        "print(labels[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftl_pHJdcZml",
        "outputId": "9d9143e6-2096-4133-be1f-e63cc1ee1c94"
      },
      "source": [
        "test_size = 0.2\n",
        "from sklearn.model_selection import train_test_split\n",
        "corpus_train, corpus_dev, labels_train, labels_dev = train_test_split(data,labels,test_size=test_size)\n",
        "print(len(corpus_train))\n",
        "print(len(corpus_dev))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46588\n",
            "11647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NRG9zmY0_5S"
      },
      "source": [
        "##### basic preprocessing for text\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def normalize_document(doc):\n",
        "\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    doc = doc.replace('\"',\"\")\n",
        "\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # re-create document from filtered tokens\n",
        "    return \" \".join(filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppIhYTr--Kgv"
      },
      "source": [
        "pro_corpus_train = [] \n",
        "for doc in corpus_train:\n",
        "  pro_corpus_train.append(normalize_document(doc))\n",
        "\n",
        "pro_corpus_dev = [] \n",
        "for doc in corpus_dev:\n",
        "  pro_corpus_dev.append(normalize_document(doc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puiO9BCH-x74",
        "outputId": "f8885788-2bcf-4078-877a-2f10918e975d"
      },
      "source": [
        "print(len(pro_corpus_train))\n",
        "print(len(pro_corpus_dev))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46588\n",
            "11647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "1bKbMsKE2REz",
        "outputId": "778d68d3-1b85-44e8-c4e3-cde43e0bc985"
      },
      "source": [
        "##### analyzing average length of the documents\n",
        "seq_len = [len(i.split()) for i in pro_corpus_train]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe2f8429748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lrUPtfG4DM6",
        "outputId": "195d9a86-afa2-4398-a51c-94ed6a6d541f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luMLvYxm3-Tz"
      },
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9MznOIK4Oqi"
      },
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCt3wC5BY6kO"
      },
      "source": [
        "c=0\n",
        "for param in bert.parameters():\n",
        "  c+=1\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aMneFpP9FwH"
      },
      "source": [
        "\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "  param.requires_grad = False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvYMHckm3s0L"
      },
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    pro_corpus_train,\n",
        "    padding = True,\n",
        "    max_length = 250,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_dev = tokenizer.batch_encode_plus(\n",
        "    pro_corpus_dev,\n",
        "    padding = True,\n",
        "    max_length = 250,\n",
        "    truncation=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd46j5mu6mP9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95jAFtFx6AMr"
      },
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'],dtype=torch.int64)\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'],dtype=torch.int64)\n",
        "train_y = torch.tensor(labels_train,dtype=torch.int64)\n",
        "\n",
        "dev_seq = torch.tensor(tokens_dev['input_ids'],dtype=torch.int64)\n",
        "dev_mask = torch.tensor(tokens_dev['attention_mask'],dtype=torch.int64)\n",
        "dev_y = torch.tensor(labels_dev,dtype=torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85wAkgtshgsb",
        "outputId": "1bf1092c-cb4e-47f7-d72d-d9dd4f15b986"
      },
      "source": [
        "print(dev_seq.shape)\n",
        "print(dev_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([11647, 250])\n",
            "torch.Size([11647, 250])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MphiWCak5-eY"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "dev_data = TensorDataset(dev_seq, dev_mask, dev_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "dev_dataloader = DataLoader(dev_data, sampler = dev_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di8EfbTr96H4",
        "outputId": "3636e2e6-2a51-4488-e3be-b5213a28261e"
      },
      "source": [
        "print(labels_train[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.0, 4.0, 3.0, 4.0, 1.0, 4.0, 3.0, 4.0, 4.0, 4.0, 3.0, 3.0, 3.0, 3.0, 2.0, 4.0, 4.0, 3.0, 4.0, 3.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuouG5XI9o8c"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      #self.lstm = nn.LSTM(input_size=768, hidden_size=256,batch_first=True)\n",
        "      self.fc2 = nn.Linear(512,64)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.out = nn.Linear(64,5)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      \n",
        "      x = self.relu(self.fc1(cls_hs))\n",
        "      x = self.relu(self.fc2(x))\n",
        "\n",
        "      #x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.out(x) \n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v36x_b8CTXv"
      },
      "source": [
        "model = BERT_Arch(bert)\n",
        "device = torch.device(\"cuda\")\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJJlH43UIGqg"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "from babel.dates import format_time\n",
        "import time\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 5e-4)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wov5UoBpIWrz"
      },
      "source": [
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  '''\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]'''\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # push the batch to gpu\n",
        "    #batch = [r.to(device) for r in batch]\n",
        "    \n",
        " \n",
        "    sent_id, mask, labels = batch[0].to(device),batch[1].to(device),batch[2].to(device)\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # add on to the total loss\n",
        "    loss_item = loss.item()\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "      print(\"loss\",loss_item)\n",
        "\n",
        "    total_loss = total_loss + loss_item\n",
        "\n",
        "    '''\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)'''\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  #total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hrpBVGCMPdB"
      },
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  t0 = time.time()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds = []\n",
        "  count=0\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(dev_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dev_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    #batch = [t.to(device) for t in batch]\n",
        "    \n",
        "\n",
        "    sent_id, mask, labels = batch[0].to(device),batch[1].to(device),batch[2].to(device)\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "      true_class = np.argmax(preds,axis=1)\n",
        "      for myvar in range(len(labels)):\n",
        "        if labels[myvar]== true_class[myvar]:\n",
        "          count+=1\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(dev_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  print(\"Validation accuracy\",count/(len(dev_dataloader)*batch_size))\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zYQjbi4MZFz",
        "outputId": "395ee1ae-9ea1-4949-b248-f6d76a5ab048"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.2580506801605225\n",
            "  Batch   100  of    728.\n",
            "loss 1.2362490892410278\n",
            "  Batch   150  of    728.\n",
            "loss 1.1422157287597656\n",
            "  Batch   200  of    728.\n",
            "loss 1.307981014251709\n",
            "  Batch   250  of    728.\n",
            "loss 1.0785303115844727\n",
            "  Batch   300  of    728.\n",
            "loss 1.1292082071304321\n",
            "  Batch   350  of    728.\n",
            "loss 1.2482842206954956\n",
            "  Batch   400  of    728.\n",
            "loss 1.1211516857147217\n",
            "  Batch   450  of    728.\n",
            "loss 1.1445361375808716\n",
            "  Batch   500  of    728.\n",
            "loss 1.1100049018859863\n",
            "  Batch   550  of    728.\n",
            "loss 1.2164679765701294\n",
            "  Batch   600  of    728.\n",
            "loss 1.0990663766860962\n",
            "  Batch   650  of    728.\n",
            "loss 1.15267014503479\n",
            "  Batch   700  of    728.\n",
            "loss 1.1828923225402832\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5554601648351648\n",
            "\n",
            "Training Loss: 1.158\n",
            "Validation Loss: 1.093\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.2087725400924683\n",
            "  Batch   100  of    728.\n",
            "loss 1.2990130186080933\n",
            "  Batch   150  of    728.\n",
            "loss 1.0930728912353516\n",
            "  Batch   200  of    728.\n",
            "loss 1.1523425579071045\n",
            "  Batch   250  of    728.\n",
            "loss 1.2230154275894165\n",
            "  Batch   300  of    728.\n",
            "loss 1.1622391939163208\n",
            "  Batch   350  of    728.\n",
            "loss 1.10614013671875\n",
            "  Batch   400  of    728.\n",
            "loss 1.0454516410827637\n",
            "  Batch   450  of    728.\n",
            "loss 0.9985622763633728\n",
            "  Batch   500  of    728.\n",
            "loss 1.1464743614196777\n",
            "  Batch   550  of    728.\n",
            "loss 1.1650264263153076\n",
            "  Batch   600  of    728.\n",
            "loss 1.126523494720459\n",
            "  Batch   650  of    728.\n",
            "loss 1.181895136833191\n",
            "  Batch   700  of    728.\n",
            "loss 1.0977836847305298\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5667067307692307\n",
            "\n",
            "Training Loss: 1.110\n",
            "Validation Loss: 1.066\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.2364822626113892\n",
            "  Batch   100  of    728.\n",
            "loss 1.1295289993286133\n",
            "  Batch   150  of    728.\n",
            "loss 1.2838395833969116\n",
            "  Batch   200  of    728.\n",
            "loss 0.9919606447219849\n",
            "  Batch   250  of    728.\n",
            "loss 0.9756743311882019\n",
            "  Batch   300  of    728.\n",
            "loss 1.2721012830734253\n",
            "  Batch   350  of    728.\n",
            "loss 1.1570135354995728\n",
            "  Batch   400  of    728.\n",
            "loss 1.0381968021392822\n",
            "  Batch   450  of    728.\n",
            "loss 1.0262216329574585\n",
            "  Batch   500  of    728.\n",
            "loss 1.084766149520874\n",
            "  Batch   550  of    728.\n",
            "loss 1.0426145792007446\n",
            "  Batch   600  of    728.\n",
            "loss 1.1491005420684814\n",
            "  Batch   650  of    728.\n",
            "loss 1.128195881843567\n",
            "  Batch   700  of    728.\n",
            "loss 0.999507486820221\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5631868131868132\n",
            "\n",
            "Training Loss: 1.093\n",
            "Validation Loss: 1.051\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 0.8848580718040466\n",
            "  Batch   100  of    728.\n",
            "loss 0.9536254405975342\n",
            "  Batch   150  of    728.\n",
            "loss 1.0609241724014282\n",
            "  Batch   200  of    728.\n",
            "loss 0.9852010607719421\n",
            "  Batch   250  of    728.\n",
            "loss 1.0099866390228271\n",
            "  Batch   300  of    728.\n",
            "loss 1.0165435075759888\n",
            "  Batch   350  of    728.\n",
            "loss 1.1320998668670654\n",
            "  Batch   400  of    728.\n",
            "loss 0.9140589237213135\n",
            "  Batch   450  of    728.\n",
            "loss 1.0819647312164307\n",
            "  Batch   500  of    728.\n",
            "loss 1.0516308546066284\n",
            "  Batch   550  of    728.\n",
            "loss 0.8704782128334045\n",
            "  Batch   600  of    728.\n",
            "loss 0.9802956581115723\n",
            "  Batch   650  of    728.\n",
            "loss 0.9469783902168274\n",
            "  Batch   700  of    728.\n",
            "loss 1.1434853076934814\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5719436813186813\n",
            "\n",
            "Training Loss: 1.080\n",
            "Validation Loss: 1.045\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.2017030715942383\n",
            "  Batch   100  of    728.\n",
            "loss 1.0556135177612305\n",
            "  Batch   150  of    728.\n",
            "loss 1.031091332435608\n",
            "  Batch   200  of    728.\n",
            "loss 0.9988070726394653\n",
            "  Batch   250  of    728.\n",
            "loss 1.207066535949707\n",
            "  Batch   300  of    728.\n",
            "loss 1.092842936515808\n",
            "  Batch   350  of    728.\n",
            "loss 1.0843737125396729\n",
            "  Batch   400  of    728.\n",
            "loss 1.1287072896957397\n",
            "  Batch   450  of    728.\n",
            "loss 1.1578013896942139\n",
            "  Batch   500  of    728.\n",
            "loss 0.9530813097953796\n",
            "  Batch   550  of    728.\n",
            "loss 1.087284803390503\n",
            "  Batch   600  of    728.\n",
            "loss 1.0798004865646362\n",
            "  Batch   650  of    728.\n",
            "loss 1.1933056116104126\n",
            "  Batch   700  of    728.\n",
            "loss 1.114653468132019\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.581301510989011\n",
            "\n",
            "Training Loss: 1.078\n",
            "Validation Loss: 1.076\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.0364024639129639\n",
            "  Batch   100  of    728.\n",
            "loss 0.9028539061546326\n",
            "  Batch   150  of    728.\n",
            "loss 1.1561390161514282\n",
            "  Batch   200  of    728.\n",
            "loss 1.0805578231811523\n",
            "  Batch   250  of    728.\n",
            "loss 0.9674639105796814\n",
            "  Batch   300  of    728.\n",
            "loss 1.1001689434051514\n",
            "  Batch   350  of    728.\n",
            "loss 1.0629627704620361\n",
            "  Batch   400  of    728.\n",
            "loss 1.2328338623046875\n",
            "  Batch   450  of    728.\n",
            "loss 1.0497407913208008\n",
            "  Batch   500  of    728.\n",
            "loss 1.0000455379486084\n",
            "  Batch   550  of    728.\n",
            "loss 1.0596089363098145\n",
            "  Batch   600  of    728.\n",
            "loss 0.9394180178642273\n",
            "  Batch   650  of    728.\n",
            "loss 0.939513623714447\n",
            "  Batch   700  of    728.\n",
            "loss 1.1261183023452759\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5722012362637363\n",
            "\n",
            "Training Loss: 1.074\n",
            "Validation Loss: 1.029\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.1255093812942505\n",
            "  Batch   100  of    728.\n",
            "loss 1.0851742029190063\n",
            "  Batch   150  of    728.\n",
            "loss 1.174944281578064\n",
            "  Batch   200  of    728.\n",
            "loss 1.0794103145599365\n",
            "  Batch   250  of    728.\n",
            "loss 1.0776695013046265\n",
            "  Batch   300  of    728.\n",
            "loss 0.9445586800575256\n",
            "  Batch   350  of    728.\n",
            "loss 1.1743956804275513\n",
            "  Batch   400  of    728.\n",
            "loss 1.1666613817214966\n",
            "  Batch   450  of    728.\n",
            "loss 1.0243321657180786\n",
            "  Batch   500  of    728.\n",
            "loss 1.1161848306655884\n",
            "  Batch   550  of    728.\n",
            "loss 0.9886699318885803\n",
            "  Batch   600  of    728.\n",
            "loss 0.9298933744430542\n",
            "  Batch   650  of    728.\n",
            "loss 0.9319722056388855\n",
            "  Batch   700  of    728.\n",
            "loss 0.8102571964263916\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5701407967032966\n",
            "\n",
            "Training Loss: 1.070\n",
            "Validation Loss: 1.041\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.0323848724365234\n",
            "  Batch   100  of    728.\n",
            "loss 1.0165096521377563\n",
            "  Batch   150  of    728.\n",
            "loss 1.0417671203613281\n",
            "  Batch   200  of    728.\n",
            "loss 1.0121115446090698\n",
            "  Batch   250  of    728.\n",
            "loss 1.1131830215454102\n",
            "  Batch   300  of    728.\n",
            "loss 1.1420073509216309\n",
            "  Batch   350  of    728.\n",
            "loss 1.065614938735962\n",
            "  Batch   400  of    728.\n",
            "loss 1.2006170749664307\n",
            "  Batch   450  of    728.\n",
            "loss 0.9688014984130859\n",
            "  Batch   500  of    728.\n",
            "loss 1.0178581476211548\n",
            "  Batch   550  of    728.\n",
            "loss 0.8664078712463379\n",
            "  Batch   600  of    728.\n",
            "loss 1.1042449474334717\n",
            "  Batch   650  of    728.\n",
            "loss 1.1376070976257324\n",
            "  Batch   700  of    728.\n",
            "loss 0.9530091881752014\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5721153846153846\n",
            "\n",
            "Training Loss: 1.067\n",
            "Validation Loss: 1.027\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.0117859840393066\n",
            "  Batch   100  of    728.\n",
            "loss 1.0997003316879272\n",
            "  Batch   150  of    728.\n",
            "loss 0.8903253078460693\n",
            "  Batch   200  of    728.\n",
            "loss 0.9627848267555237\n",
            "  Batch   250  of    728.\n",
            "loss 1.1527912616729736\n",
            "  Batch   300  of    728.\n",
            "loss 0.9495048522949219\n",
            "  Batch   350  of    728.\n",
            "loss 1.1340776681900024\n",
            "  Batch   400  of    728.\n",
            "loss 1.1312365531921387\n",
            "  Batch   450  of    728.\n",
            "loss 1.0410839319229126\n",
            "  Batch   500  of    728.\n",
            "loss 1.1558949947357178\n",
            "  Batch   550  of    728.\n",
            "loss 1.1276516914367676\n",
            "  Batch   600  of    728.\n",
            "loss 0.9832408428192139\n",
            "  Batch   650  of    728.\n",
            "loss 1.0898926258087158\n",
            "  Batch   700  of    728.\n",
            "loss 1.0479240417480469\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n",
            "  Batch   100  of    182.\n",
            "  Batch   150  of    182.\n",
            "Validation accuracy 0.5843063186813187\n",
            "\n",
            "Training Loss: 1.065\n",
            "Validation Loss: 1.033\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    728.\n",
            "loss 1.3271799087524414\n",
            "  Batch   100  of    728.\n",
            "loss 1.0559780597686768\n",
            "  Batch   150  of    728.\n",
            "loss 1.0653456449508667\n",
            "  Batch   200  of    728.\n",
            "loss 1.0542722940444946\n",
            "  Batch   250  of    728.\n",
            "loss 1.36472749710083\n",
            "  Batch   300  of    728.\n",
            "loss 0.9626333713531494\n",
            "  Batch   350  of    728.\n",
            "loss 1.0554828643798828\n",
            "  Batch   400  of    728.\n",
            "loss 1.1387429237365723\n",
            "  Batch   450  of    728.\n",
            "loss 1.0039979219436646\n",
            "  Batch   500  of    728.\n",
            "loss 1.2037932872772217\n",
            "  Batch   550  of    728.\n",
            "loss 1.0522831678390503\n",
            "  Batch   600  of    728.\n",
            "loss 0.968580961227417\n",
            "  Batch   650  of    728.\n",
            "loss 1.1757436990737915\n",
            "  Batch   700  of    728.\n",
            "loss 1.047506332397461\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    182.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-508905871e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-d0e09e9925d7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# iterate over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Progress update every 50 batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbDYymf1HM7U"
      },
      "source": [
        "!pip install \"tensorflow>=1.7.0\"\n",
        "!pip install tensorflow-hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b4t9gReAEUm"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqthikkIACGJ"
      },
      "source": [
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "url = \"https://tfhub.dev/google/elmo/3\"\n",
        "embed =hub.Module(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPmymblsundU"
      },
      "source": [
        "list_train = [pro_corpus_train[i:i+10] for i in range(0,pro_corpus_train.shape[0],10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Afy-p1zcft"
      },
      "source": [
        "print(len(list_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9D3N2GpvyC5"
      },
      "source": [
        "def elmo_vectors(x):\n",
        "  embeddings = embed(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    sess.run(tf.compat.v1.tables_initializer())\n",
        "    # return average of ELMo features\n",
        "    return sess.run(tf.reduce_mean(embeddings,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvrVl9goxa_A"
      },
      "source": [
        "elmo_train = [elmo_vectors(x) for x in list_train]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}